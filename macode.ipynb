{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0t5k9KRxEO6Zjc7d3/MDp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oixchi/mathesis/blob/main/macode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRAgIP8zTr_R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check if a GPU is available and if not, use a CPU\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "metadata": {
        "id": "2hgV3sf2UIVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nUNn8iC_UKxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "print(f\"Function call started at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
        "\n",
        "# Place your code here\n",
        "time.sleep(10)  # Replace with model training process\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "print(f\"Function call ended at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "elapsed_minutes = elapsed_time / 60\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Total time taken: {elapsed_time:.2f} seconds ({elapsed_minutes:.2f} minutes)\")\n"
      ],
      "metadata": {
        "id": "6IzuvgRmUNJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "model_name= 'gpt2-large'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "model.eval()\n",
        "def generate_text(prompt, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=5,\n",
        "            early_stopping=False,\n",
        "            pad_token_id=model.config.eos_token_id,\n",
        "            attention_mask=inputs['attention_mask']\n",
        "        )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "promptk = \"Once upon a time\"\n",
        "start_time = time.time()\n",
        "print(f\"Function call started at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
        "generated_text = generate_text(promptk)\n",
        "end_time = time.time()\n",
        "print(f\"Function call ended at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
        "print(\"BEGIN------------------\", generated_text, \"-------------END\")\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "elapsed_minutes = elapsed_time / 60\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Total time taken: {elapsed_time:.2f} seconds ({elapsed_minutes:.2f} minutes)\")\n"
      ],
      "metadata": {
        "id": "wj29RSYuUREG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "id": "LdptItw1U7Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the pre-trained embeddings model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define your text (a sentence or a list of sentences)\n",
        "text = [\"This is an example sentence.\", \"Embeddings convert text into vectors.\"]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(text)\n",
        "\n",
        "# Show the embedding for the first sentence\n",
        "print(embeddings[0])"
      ],
      "metadata": {
        "id": "LlYPQ_otU4uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "CTgFZHqAOc2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Title of the app\n",
        "st.title(\"My First Streamlit App\")\n",
        "# Text input\n",
        "name = st.text_input(\"Enter your name:\")\n",
        "# Display a greeting message\n",
        "if st.button(\"Submit\"):\n",
        "    st.write(f\"Hello, {name}! Welcome to Streamlit!\")"
      ],
      "metadata": {
        "id": "4g-0Agj3OQf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "\n",
        "ngrok.set_auth_token(\"2n9kxxKgkykAbfQ0AtCbstcsSgC_2JvQe6E6f9dDbbfspXsBU\")\n",
        "\n",
        "# Create the Streamlit app\n",
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"My Streamlit App on Google Colab\")\n",
        "\n",
        "name = st.text_input(\"Enter your name:\")\n",
        "if st.button(\"Submit\"):\n",
        "    st.write(f\"Hello, {name}!\")\n",
        "\"\"\"\n",
        "\n",
        "# Write the app to a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Set up ngrok to tunnel to port 8501\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(\"8501\", \"http\")\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&\n"
      ],
      "metadata": {
        "id": "FeZOx-3WPFj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ngrok.set_auth_token(\"2n9kxxKgkykAbfQ0AtCbstcsSgC_2JvQe6E6f9dDbbfspXsBU\")\n",
        "\n",
        "# Create the Streamlit app\n",
        "app_code = \"\"\"\n",
        "# Create centered main title\n",
        "st.title('Ask me a question K')\n",
        "\n",
        "# Chat message storage\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    st.chat_message(message[\"role\"]).markdown(message['content'])\n",
        "\n",
        "prompt = st.chat_input(\"Input your prompt here\")\n",
        "\n",
        "if prompt:\n",
        "    st.chat_message('user').markdown(prompt)\n",
        "    st.session_state.messages.append({'role':'user', 'content':prompt})\n",
        "    #response=generate_response(prompt)\n",
        "    #response = retrieve(prompt, k=3)\n",
        "\tresponse = \"Hallo Ketli\"\n",
        "    st.chat_message('assistant').markdown(response)\n",
        "    st.session_state.messages.append({'role':'assistant', 'content':response})\n",
        "\"\"\"\n",
        "\n",
        "# Write the app to a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Set up ngrok to tunnel to port 8501\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(\"8501\", \"http\")\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8unQNN90RRa_",
        "outputId": "b3a6588a-92c8-402c-aaab-7066f86e91f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://bfc5-34-124-199-28.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "nxyR2jYERg04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "xkWqxXKzRobO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "u9s1HKxNRxM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import streamlit as st\n"
      ],
      "metadata": {
        "id": "T_VeWaY3SGRI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # This will prompt you to upload 'test.pdf'\n",
        "\n",
        "# Step 2: Check the current directory\n",
        "import os\n",
        "\n",
        "print(\"Files in current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "0d9KoalvSoPz",
        "outputId": "eb50d1bd-1190-45f6-c5b1-c2b990f9c187"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c11cdaa8-a447-42f9-a4a8-3ea039a87585\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c11cdaa8-a447-42f9-a4a8-3ea039a87585\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ebrains_dataset_new.pdf to ebrains_dataset_new.pdf\n",
            "Files in current directory:\n",
            "['.config', 'ebrains_dataset_new.pdf', 'app.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = 'ebrains_dataset_new.pdf'\n",
        "pdf_document = fitz.open(pdf_path)"
      ],
      "metadata": {
        "id": "_fysZmiXSwdj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract text from each page\n",
        "pdf_text = []\n",
        "for page_num in range(pdf_document.page_count):\n",
        "    page = pdf_document.load_page(page_num)\n",
        "    pdf_text.append(page.get_text())\n",
        "\n",
        "# Close the PDF document\n",
        "pdf_document.close()\n",
        "\n",
        "# Join all the extracted text into a single string or split into paragraphs/sentences\n",
        "pdf_text = \" \".join(pdf_text)"
      ],
      "metadata": {
        "id": "3pssriHFS1xb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to chunk the text into manageable pieces\n",
        "def chunk_text(text, max_chunk_size=512, overlap=50):\n",
        "    sentences = nltk.sent_tokenize(text)  # Split into sentences\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_chunk_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence.split())\n",
        "\n",
        "        # Check if adding this sentence exceeds the max chunk size\n",
        "        if current_chunk_size + sentence_length > max_chunk_size:\n",
        "            # Append the current chunk to chunks and reset\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            # Start a new chunk with overlap\n",
        "            current_chunk = current_chunk[-overlap:]  # Keep the last few sentences for context\n",
        "            current_chunk_size = sum(len(s.split()) for s in current_chunk)\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_chunk.append(sentence)\n",
        "        current_chunk_size += sentence_length\n",
        "\n",
        "    # Add any remaining sentences as the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Split the text into sentences\n",
        "# documents = nltk.sent_tokenize(pdf_text)\n",
        "\n",
        "# Split the text into chunks\n",
        "documents = chunk_text(pdf_text, max_chunk_size=512, overlap=50)\n",
        "\n",
        "# Alternatively, you can split it into paragraphs\n",
        "# documents = pdf_text.split('\\n\\n')  # Split by double newlines (for paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-QhK2s0Te3r",
        "outputId": "d73d7bc2-7d9c-41a3-babb-cb22e0c6715a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Create document embeddings\n",
        "document_embeddings = embedding_model.encode(documents)\n",
        "\n",
        "# Convert the embeddings to a format suitable for FAISS\n",
        "embedding_dim = document_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Add the document embeddings to the index\n",
        "faiss_index.add(np.array(document_embeddings))\n",
        "\n",
        "def retrieve(query, k=5):\n",
        "    # Encode the query into a vector\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Search FAISS index for the most relevant documents\n",
        "    distances, indices = faiss_index.search(query_embedding, k)\n",
        "    return [documents[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "Wujcs76yTfxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"what is virtual brain\"\n",
        "retrieve(prompt, k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glM_FpaOUcJs",
        "outputId": "7b618549-7858-4f97-d0d0-b86a8585f1a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NEURON’s computational engine employs special algorithms that achieve high efficiency by exploiting \\nthe structure of the equations that describe neuronal properties. It has functions that are tailored for \\nconveniently controlling simulations, and presenting the results of real neurophysiological problems \\ngraphically in ways that are quickly and intuitively grasped. Instead of forcing users to reformulate their \\nconceptual models to fit the requirements of a general purpose simulator, NEURON is designed to let \\nthem deal directly with familiar neuroscience concepts. Consequently, users can think in terms of the \\nbiophysical properties of membrane and cytoplasm, the branched architecture of neurons, and the effects\\nof synaptic communication between cells. The Virtual Brain (TVB) is an open-source platform for constructing and simulating personalised brain \\nnetwork models. The TVB-on-EBRAINS ecosystem includes a variety of prepackaged modules, \\nintegrated simulation tools, pipelines and data sets for easy and immediate use on EBRAINS. Process \\nyour large cohort databases and use these results to develop potential medical treatments, therapies or \\ndiagnostic procedures. The Virtual Brain  \\nThe Virtual Brain (TVB) is an open-source platform for constructing and simulating personalised brain \\nnetwork models. The TVB-on-EBRAINS ecosystem includes a variety of prepackaged modules, \\nintegrated simulation tools, pipelines and data sets for easy and immediate use on EBRAINS. Process \\nyour large cohort databases and use these results to develop potential medical treatments, therapies or \\ndiagnostic procedures. NEST is a simulator for spiking neural network models that focuses on the dynamics, size and structure of\\nneural systems, rather than on the exact morphology of individual neurons. It is ideal for networks of any \\nsize, including models of information processing (e.g. in the visual or auditory cortex of mammals), \\nmodels of network activity dynamics (e.g. laminar cortical networks or balanced random networks) and \\nmodels of learning and plasticity. NEST is openly available for download. NEST  \\nNEST is a simulator for spiking neural network models that focuses on the dynamics, size and structure of\\nneural systems, rather than on the exact morphology of individual neurons. It is ideal for networks of any \\nsize, including models of information processing (e.g. in the visual or auditory cortex of mammals), \\nmodels of network activity dynamics (e.g. laminar cortical networks or balanced random networks) and \\nmodels of learning and plasticity. NEST is openly available for download. NEURON’s computational engine employs special algorithms that achieve high efficiency by exploiting \\nthe structure of the equations that describe neuronal properties. It has functions that are tailored for \\nconveniently controlling simulations, and presenting the results of real neurophysiological problems \\ngraphically in ways that are quickly and intuitively grasped. Instead of forcing users to reformulate their \\nconceptual models to fit the requirements of a general purpose simulator, NEURON is designed to let \\nthem deal directly with familiar neuroscience concepts. Consequently, users can think in terms of the \\nbiophysical properties of membrane and cytoplasm, the branched architecture of neurons, and the effects\\nof synaptic communication between cells. Neuron  \\nNEURON’s computational engine employs special algorithms that achieve high efficiency by exploiting \\nthe structure of the equations that describe neuronal properties. It has functions that are tailored for \\nconveniently controlling simulations, and presenting the results of real neurophysiological problems \\ngraphically in ways that are quickly and intuitively grasped. Instead of forcing users to reformulate their \\nconceptual models to fit the requirements of a general purpose simulator, NEURON is designed to let \\nthem deal directly with familiar neuroscience concepts. Consequently, users can think in terms of the \\nbiophysical properties of membrane and cytoplasm, the branched architecture of neurons, and the effects\\nof synaptic communication between cells. News  \\nUsing EBRAINS modelling tools to investigate the relationship between brain structure and function  \\nScientists use EBRAINS to simulate deep brain stimulation in Parkinsonâ\\x00\\x00s disease  \\nHow scientists are changing the way we treat epilepsy with EBRAINS  \\nEBRAINS is open and free. Sign up now for complete access to our tools and services. EBRAINS is open and free. Sign up now for complete access to our tools and services. Make the most out of EBRAINS  \\nEBRAINS is open and free. Sign up now for complete access to our tools and services. Follow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nEBRAINS is funded by the Horizon Europe Framework Programme. EBRAINS is funded by the Horizon Europe Framework Programme. EBRAINS is funded by the Horizon Europe Framework Programme. EBRAINS is funded by the Horizon Europe Framework Programme. The formal establishment of EBRAINS BELGIUM as a consortium and an EBRAINS National Node is \\ncurrently in process. To date, UHasselt currently acts as the Full Member within Belgium. The Belgian members of EBRAINS AISBL excel in imaging/EEG analyses, atlasing, modeling and \\nsensitive data handling. Together, we strive towards laying the foundations for EBRAINS BELGIUM to \\nbecome part of a pan-European open state-of-the-art distributed Research Infrastructure (RI) that fosters \\ncollaborative brain science, opens the way to ground-breaking discovery in neuroscience, and aims to aid\\nto secure Europeâ\\x00\\x00s leading position in the dynamically growing field of multidisciplinary brain research \\nand its exploitation. Moreover, training, professional development, and community building will become \\nkey in our portfolio, since a robust, high-quality EBRAINS BELGIUM training platform will:  \\nEBRAINS BELGIUM aspires to foster interactions between all Belgian Universities, co-developing and \\nusing tools that can be used across species, including healthy humans, patients, and animals. Do you have any questions about the Belgium node? Please submit your question and email address \\nbelow. EBRAINS is open and free. Sign up now for complete access to our tools and services. Follow EBRAINS to keep up-to-date  \\nEBRAINS is funded by the Horizon Europe Framework Programme. The formal establishment of EBRAINS BELGIUM as a consortium and an EBRAINS National Node is \\ncurrently in process. To date, UHasselt currently acts as the Full Member within Belgium. The Belgian members of EBRAINS AISBL excel in imaging/EEG analyses, atlasing, modeling and \\nsensitive data handling.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}