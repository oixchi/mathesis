{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwQsSsZA/cnRw2EbtwRlz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oixchi/mathesis/blob/main/macode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRAgIP8zTr_R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check if a GPU is available and if not, use a CPU\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "metadata": {
        "id": "2hgV3sf2UIVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nUNn8iC_UKxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "print(f\"Function call started at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
        "\n",
        "# Place your code here\n",
        "time.sleep(10)  # Replace with model training process\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "print(f\"Function call ended at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "elapsed_minutes = elapsed_time / 60\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Total time taken: {elapsed_time:.2f} seconds ({elapsed_minutes:.2f} minutes)\")\n"
      ],
      "metadata": {
        "id": "6IzuvgRmUNJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "model_name= 'gpt2-large'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "model.eval()\n",
        "def generate_text(prompt, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=5,\n",
        "            early_stopping=False,\n",
        "            pad_token_id=model.config.eos_token_id,\n",
        "            attention_mask=inputs['attention_mask']\n",
        "        )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "promptk = \"Once upon a time\"\n",
        "start_time = time.time()\n",
        "print(f\"Function call started at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
        "generated_text = generate_text(promptk)\n",
        "end_time = time.time()\n",
        "print(f\"Function call ended at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
        "print(\"BEGIN------------------\", generated_text, \"-------------END\")\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "elapsed_minutes = elapsed_time / 60\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Total time taken: {elapsed_time:.2f} seconds ({elapsed_minutes:.2f} minutes)\")\n"
      ],
      "metadata": {
        "id": "wj29RSYuUREG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "id": "LdptItw1U7Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the pre-trained embeddings model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define your text (a sentence or a list of sentences)\n",
        "text = [\"This is an example sentence.\", \"Embeddings convert text into vectors.\"]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(text)\n",
        "\n",
        "# Show the embedding for the first sentence\n",
        "print(embeddings[0])"
      ],
      "metadata": {
        "id": "LlYPQ_otU4uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "CTgFZHqAOc2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Title of the app\n",
        "st.title(\"My First Streamlit App\")\n",
        "# Text input\n",
        "name = st.text_input(\"Enter your name:\")\n",
        "# Display a greeting message\n",
        "if st.button(\"Submit\"):\n",
        "    st.write(f\"Hello, {name}! Welcome to Streamlit!\")"
      ],
      "metadata": {
        "id": "4g-0Agj3OQf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "\n",
        "ngrok.set_auth_token(\"2n9kxxKgkykAbfQ0AtCbstcsSgC_2JvQe6E6f9dDbbfspXsBU\")\n",
        "\n",
        "# Create the Streamlit app\n",
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"My Streamlit App on Google Colab\")\n",
        "\n",
        "name = st.text_input(\"Enter your name:\")\n",
        "if st.button(\"Submit\"):\n",
        "    st.write(f\"Hello, {name}!\")\n",
        "\"\"\"\n",
        "\n",
        "# Write the app to a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Set up ngrok to tunnel to port 8501\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(\"8501\", \"http\")\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&\n"
      ],
      "metadata": {
        "id": "FeZOx-3WPFj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ngrok.set_auth_token(\"2n9kxxKgkykAbfQ0AtCbstcsSgC_2JvQe6E6f9dDbbfspXsBU\")\n",
        "\n",
        "# Create the Streamlit app\n",
        "app_code = \"\"\"\n",
        "# Create centered main title\n",
        "st.title('Ask me a question K')\n",
        "\n",
        "# Chat message storage\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    st.chat_message(message[\"role\"]).markdown(message['content'])\n",
        "\n",
        "prompt = st.chat_input(\"Input your prompt here\")\n",
        "\n",
        "if prompt:\n",
        "    st.chat_message('user').markdown(prompt)\n",
        "    st.session_state.messages.append({'role':'user', 'content':prompt})\n",
        "    #response=generate_response(prompt)\n",
        "    #response = retrieve(prompt, k=3)\n",
        "\tresponse = \"Hallo Ketli\"\n",
        "    st.chat_message('assistant').markdown(response)\n",
        "    st.session_state.messages.append({'role':'assistant', 'content':response})\n",
        "\"\"\"\n",
        "\n",
        "# Write the app to a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Set up ngrok to tunnel to port 8501\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(\"8501\", \"http\")\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8unQNN90RRa_",
        "outputId": "b3a6588a-92c8-402c-aaab-7066f86e91f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://bfc5-34-124-199-28.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "nxyR2jYERg04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "xkWqxXKzRobO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "u9s1HKxNRxM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import streamlit as st\n"
      ],
      "metadata": {
        "id": "T_VeWaY3SGRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # This will prompt you to upload 'test.pdf'\n",
        "\n",
        "# Step 2: Check the current directory\n",
        "import os\n",
        "\n",
        "print(\"Files in current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "0d9KoalvSoPz",
        "outputId": "eb50d1bd-1190-45f6-c5b1-c2b990f9c187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c11cdaa8-a447-42f9-a4a8-3ea039a87585\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c11cdaa8-a447-42f9-a4a8-3ea039a87585\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ebrains_dataset_new.pdf to ebrains_dataset_new.pdf\n",
            "Files in current directory:\n",
            "['.config', 'ebrains_dataset_new.pdf', 'app.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = 'ebrains_dataset_new.pdf'\n",
        "pdf_document = fitz.open(pdf_path)"
      ],
      "metadata": {
        "id": "_fysZmiXSwdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract text from each page\n",
        "pdf_text = []\n",
        "for page_num in range(pdf_document.page_count):\n",
        "    page = pdf_document.load_page(page_num)\n",
        "    pdf_text.append(page.get_text())\n",
        "\n",
        "# Close the PDF document\n",
        "pdf_document.close()\n",
        "\n",
        "# Join all the extracted text into a single string or split into paragraphs/sentences\n",
        "pdf_text = \" \".join(pdf_text)"
      ],
      "metadata": {
        "id": "3pssriHFS1xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to chunk the text into manageable pieces\n",
        "def chunk_text(text, max_chunk_size=512, overlap=50):\n",
        "    sentences = nltk.sent_tokenize(text)  # Split into sentences\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_chunk_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence.split())\n",
        "\n",
        "        # Check if adding this sentence exceeds the max chunk size\n",
        "        if current_chunk_size + sentence_length > max_chunk_size:\n",
        "            # Append the current chunk to chunks and reset\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            # Start a new chunk with overlap\n",
        "            current_chunk = current_chunk[-overlap:]  # Keep the last few sentences for context\n",
        "            current_chunk_size = sum(len(s.split()) for s in current_chunk)\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_chunk.append(sentence)\n",
        "        current_chunk_size += sentence_length\n",
        "\n",
        "    # Add any remaining sentences as the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Split the text into sentences\n",
        "# documents = nltk.sent_tokenize(pdf_text)\n",
        "\n",
        "# Split the text into chunks\n",
        "documents = chunk_text(pdf_text, max_chunk_size=512, overlap=50)\n",
        "\n",
        "# Alternatively, you can split it into paragraphs\n",
        "# documents = pdf_text.split('\\n\\n')  # Split by double newlines (for paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-QhK2s0Te3r",
        "outputId": "d73d7bc2-7d9c-41a3-babb-cb22e0c6715a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Create document embeddings\n",
        "document_embeddings = embedding_model.encode(documents)\n",
        "\n",
        "# Convert the embeddings to a format suitable for FAISS\n",
        "embedding_dim = document_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Add the document embeddings to the index\n",
        "faiss_index.add(np.array(document_embeddings))\n",
        "\n",
        "def retrieve(query, k=5):\n",
        "    # Encode the query into a vector\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Search FAISS index for the most relevant documents\n",
        "    distances, indices = faiss_index.search(query_embedding, k)\n",
        "    return [documents[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "Wujcs76yTfxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"what is virtual brain\"\n",
        "retrieve(prompt, k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glM_FpaOUcJs",
        "outputId": "7b618549-7858-4f97-d0d0-b86a8585f1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NEURON’s computational engine employs special algorithms that achieve high efficiency by exploiting \\nthe structure of the equations that describe neuronal properties. It has functions that are tailored for \\nconveniently controlling simulations, and presenting the results of real neurophysiological problems \\ngraphically in ways that are quickly and intuitively grasped. Instead of forcing users to reformulate their \\nconceptual models to fit the requirements of a general purpose simulator, NEURON is designed to let \\nthem deal directly with familiar neuroscience concepts. Consequently, users can think in terms of the \\nbiophysical properties of membrane and cytoplasm, the branched architecture of neurons, and the effects\\nof synaptic communication between cells. The Virtual Brain (TVB) is an open-source platform for constructing and simulating personalised brain \\nnetwork models. The TVB-on-EBRAINS ecosystem includes a variety of prepackaged modules, \\nintegrated simulation tools, pipelines and data sets for easy and immediate use on EBRAINS. Process \\nyour large cohort databases and use these results to develop potential medical treatments, therapies or \\ndiagnostic procedures. The Virtual Brain  \\nThe Virtual Brain (TVB) is an open-source platform for constructing and simulating personalised brain \\nnetwork models. The TVB-on-EBRAINS ecosystem includes a variety of prepackaged modules, \\nintegrated simulation tools, pipelines and data sets for easy and immediate use on EBRAINS. Process \\nyour large cohort databases and use these results to develop potential medical treatments, therapies or \\ndiagnostic procedures. NEST is a simulator for spiking neural network models that focuses on the dynamics, size and structure of\\nneural systems, rather than on the exact morphology of individual neurons. It is ideal for networks of any \\nsize, including models of information processing (e.g. in the visual or auditory cortex of mammals), \\nmodels of network activity dynamics (e.g. laminar cortical networks or balanced random networks) and \\nmodels of learning and plasticity. NEST is openly available for download. NEST  \\nNEST is a simulator for spiking neural network models that focuses on the dynamics, size and structure of\\nneural systems, rather than on the exact morphology of individual neurons. It is ideal for networks of any \\nsize, including models of information processing (e.g. in the visual or auditory cortex of mammals), \\nmodels of network activity dynamics (e.g. laminar cortical networks or balanced random networks) and \\nmodels of learning and plasticity. NEST is openly available for download. NEURON’s computational engine employs special algorithms that achieve high efficiency by exploiting \\nthe structure of the equations that describe neuronal properties. It has functions that are tailored for \\nconveniently controlling simulations, and presenting the results of real neurophysiological problems \\ngraphically in ways that are quickly and intuitively grasped. Instead of forcing users to reformulate their \\nconceptual models to fit the requirements of a general purpose simulator, NEURON is designed to let \\nthem deal directly with familiar neuroscience concepts. Consequently, users can think in terms of the \\nbiophysical properties of membrane and cytoplasm, the branched architecture of neurons, and the effects\\nof synaptic communication between cells. Neuron  \\nNEURON’s computational engine employs special algorithms that achieve high efficiency by exploiting \\nthe structure of the equations that describe neuronal properties. It has functions that are tailored for \\nconveniently controlling simulations, and presenting the results of real neurophysiological problems \\ngraphically in ways that are quickly and intuitively grasped. Instead of forcing users to reformulate their \\nconceptual models to fit the requirements of a general purpose simulator, NEURON is designed to let \\nthem deal directly with familiar neuroscience concepts. Consequently, users can think in terms of the \\nbiophysical properties of membrane and cytoplasm, the branched architecture of neurons, and the effects\\nof synaptic communication between cells. News  \\nUsing EBRAINS modelling tools to investigate the relationship between brain structure and function  \\nScientists use EBRAINS to simulate deep brain stimulation in Parkinsonâ\\x00\\x00s disease  \\nHow scientists are changing the way we treat epilepsy with EBRAINS  \\nEBRAINS is open and free. Sign up now for complete access to our tools and services. EBRAINS is open and free. Sign up now for complete access to our tools and services. Make the most out of EBRAINS  \\nEBRAINS is open and free. Sign up now for complete access to our tools and services. Follow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nFollow EBRAINS to keep up-to-date  \\nEBRAINS is funded by the Horizon Europe Framework Programme. EBRAINS is funded by the Horizon Europe Framework Programme. EBRAINS is funded by the Horizon Europe Framework Programme. EBRAINS is funded by the Horizon Europe Framework Programme. The formal establishment of EBRAINS BELGIUM as a consortium and an EBRAINS National Node is \\ncurrently in process. To date, UHasselt currently acts as the Full Member within Belgium. The Belgian members of EBRAINS AISBL excel in imaging/EEG analyses, atlasing, modeling and \\nsensitive data handling. Together, we strive towards laying the foundations for EBRAINS BELGIUM to \\nbecome part of a pan-European open state-of-the-art distributed Research Infrastructure (RI) that fosters \\ncollaborative brain science, opens the way to ground-breaking discovery in neuroscience, and aims to aid\\nto secure Europeâ\\x00\\x00s leading position in the dynamically growing field of multidisciplinary brain research \\nand its exploitation. Moreover, training, professional development, and community building will become \\nkey in our portfolio, since a robust, high-quality EBRAINS BELGIUM training platform will:  \\nEBRAINS BELGIUM aspires to foster interactions between all Belgian Universities, co-developing and \\nusing tools that can be used across species, including healthy humans, patients, and animals. Do you have any questions about the Belgium node? Please submit your question and email address \\nbelow. EBRAINS is open and free. Sign up now for complete access to our tools and services. Follow EBRAINS to keep up-to-date  \\nEBRAINS is funded by the Horizon Europe Framework Programme. The formal establishment of EBRAINS BELGIUM as a consortium and an EBRAINS National Node is \\ncurrently in process. To date, UHasselt currently acts as the Full Member within Belgium. The Belgian members of EBRAINS AISBL excel in imaging/EEG analyses, atlasing, modeling and \\nsensitive data handling.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "def get_ebrains_links(base_url):\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    all_links = soup.find_all('a', href=True)\n",
        "    filtered_links = []\n",
        "\n",
        "    for link in all_links:\n",
        "        href = link['href']\n",
        "        full_url = urljoin(base_url, href)\n",
        "        if 'ebrains.eu' in full_url:\n",
        "            filtered_links.append(full_url)\n",
        "    unique_links = list(set(filtered_links))\n",
        "    return unique_links\n",
        "\n",
        "base_url = 'https://www.ebrains.eu/'\n",
        "ebrains_links = sorted(get_ebrains_links(base_url))\n",
        "print(ebrains_links)\n",
        "\n",
        "for link in ebrains_links:\n",
        "    print(get_ebrains_links(link))\n",
        "\n"
      ],
      "metadata": {
        "id": "eaVfdJEgax9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def get_ebrains_links(base_url):\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    all_links = soup.find_all('a', href=True)\n",
        "\n",
        "    # Use list comprehension to filter and process links\n",
        "    filtered_links = [urljoin(base_url, link['href']) for link in all_links if 'ebrains.eu' in urljoin(base_url, link['href'])]\n",
        "\n",
        "    # Remove duplicates using set, then sort the list\n",
        "    unique_links = sorted(list(set(filtered_links)))\n",
        "    return unique_links\n",
        "\n",
        "base_url = 'https://www.ebrains.eu/'\n",
        "\n",
        "# Get all the links from the base URL\n",
        "ebrains_links = get_ebrains_links(base_url)\n",
        "\n",
        "# Create a combined list and ensure uniqueness across all links\n",
        "all_links = set(ebrains_links)  # Use a set to start with unique base links\n",
        "\n",
        "# Gather links from all individual pages and update the set\n",
        "for link in ebrains_links:\n",
        "    page_links = get_ebrains_links(link)\n",
        "    all_links.update(page_links)  # Add new links to the set, ensuring uniqueness\n",
        "\n",
        "# Convert set back to a list and sort it\n",
        "all_links = sorted(list(all_links))\n",
        "\n",
        "# Print each link with a line break\n",
        "for link in all_links:\n",
        "    print(link)\n",
        "    print()  # Print a blank line for separation\n"
      ],
      "metadata": {
        "id": "ls0kCpZEeKGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def gather_all_ebrains_links(base_url):\n",
        "    # Function to gather and return all unique links from a base URL and its linked pages\n",
        "    def get_ebrains_links(url):\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Use list comprehension to filter and process links\n",
        "        filtered_links = [urljoin(url, link['href']) for link in all_links if 'ebrains.eu' in urljoin(url, link['href'])]\n",
        "\n",
        "        # Remove duplicates using set, then sort the list\n",
        "        unique_links = sorted(list(set(filtered_links)))\n",
        "        return unique_links\n",
        "\n",
        "    # Get all the links from the base URL\n",
        "    ebrains_links = get_ebrains_links(base_url)\n",
        "\n",
        "    # Create a combined list and ensure uniqueness across all links\n",
        "    all_links = set(ebrains_links)  # Use a set to start with unique base links\n",
        "\n",
        "    # Gather links from all individual pages and update the set\n",
        "    for link in ebrains_links:\n",
        "        page_links = get_ebrains_links(link)\n",
        "        all_links.update(page_links)  # Add new links to the set, ensuring uniqueness\n",
        "\n",
        "    # Convert set back to a list and sort it\n",
        "    return sorted(list(all_links))\n",
        "\n",
        "# Call the function and print the results\n",
        "base_url = 'https://www.ebrains.eu/'\n",
        "all_ebrains_links = gather_all_ebrains_links(base_url)\n",
        "all_ebrains_links_without_pdf = [link for link in all_ebrains_links if not link.endswith('.pdf')]\n",
        "# pdf can be used\n",
        "all_ebrains_links_with_pdf = [link for link in all_ebrains_links if link.endswith('.pdf')]\n",
        "\n",
        "for link in all_ebrains_links_with_pdf:\n",
        "    print(link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzvtElorfx1w",
        "outputId": "95558bbb-89b2-44c9-c546-627141bd811f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://files.ebrains.eu/file/1915ab0c-b800-46ca-b433-75f0e7d07dcf/ProjectCommunicationsContentOfficer-Posting.pdf\n",
            "https://files.ebrains.eu/file/1d392941-e722-4474-badd-6f811c1673fc/EBRAINS_ScientificVision_April_2023.pdf\n",
            "https://files.ebrains.eu/file/2a7a23d8-a21c-4927-8665-f530d6a31d3a/Membershipleaflet_EBRAINS_September_2024.pdf\n",
            "https://files.ebrains.eu/file/36905541-c68a-47f3-9ecc-42e36d80bce5/EBRAINS_Science_and_Technology_Commitee_To_R_e6dfe1a918.pdf\n",
            "https://files.ebrains.eu/file/3b8a3859-74e2-48a7-9882-79935c19db68/Discover_EBRAINS_2023.pdf\n",
            "https://files.ebrains.eu/file/3e7cd7a2-f9d3-4d62-9ce2-0edd81f92523/EBRAINS-Management-Board-May-2024.pdf\n",
            "https://files.ebrains.eu/file/69456c0d-de07-495a-8c8f-d47534d95223/EBRAINS-Invitation-to-tender.pdf\n",
            "https://files.ebrains.eu/file/6c5cf3b7-5630-486e-b70e-ee062d8492c4/EBRAINS-EESC-May-2024.pdf\n",
            "https://files.ebrains.eu/file/72e3d51f-a16c-41c1-b529-aca096dabb65/EBRAINS_Ethics_and_Society_Vision_June_2022_9c012269d6.pdf\n",
            "https://files.ebrains.eu/file/82d095cb-31ae-45b7-9635-c57687f03735/Gender_Equality_Plan_26012022_v1_71ccaa45a7.pdf\n",
            "https://files.ebrains.eu/file/87d4c36c-3755-4242-81f8-770fcefa3d51/EBRAINS-National-Node-Board-May-2023.pdf\n",
            "https://files.ebrains.eu/file/8d2df951-7430-4f4f-be3a-4eee60db8a9c/EBRAINS_Science_and_Technology_Committee_List_of_Members_May_2023.pdf\n",
            "https://files.ebrains.eu/file/a29a2d69-bd87-4486-92a1-95a48639cb4d/EBRAINS-Governing-Board-April-2024.pdf\n",
            "https://files.ebrains.eu/file/cab90591-f55c-4d40-a58f-2d81328a0db8/EBRAINS-Members-24-June-2024.pdf\n",
            "https://files.ebrains.eu/file/e4b05476-d2f0-49c2-8b45-41f9c317892e/EBRAINS_AISBL_Access_Policy_04_2022_85c8e61216.pdf\n",
            "https://files.ebrains.eu/file/e4b05476-d2f0-49c2-8b45-41f9c317892e/EBRAINS_Data_Provision_Protocol_dfe0dcb104.pdf\n",
            "https://files.ebrains.eu/file/e4b05476-d2f0-49c2-8b45-41f9c317892e/EBRAINS_Data_Use_Agreement_90858e7836_ef3ee29d50.pdf\n",
            "https://files.ebrains.eu/file/e4b05476-d2f0-49c2-8b45-41f9c317892e/EBRAINS_General_Terms_of_use_e457353c1a_d2122f84c2.pdf\n",
            "https://files.ebrains.eu/file/e4b05476-d2f0-49c2-8b45-41f9c317892e/EBRAINS_Privacy_Statement_2022_80958229c5.pdf\n",
            "https://files.ebrains.eu/file/e4b05476-d2f0-49c2-8b45-41f9c317892e/EBRAINS_Website_Cookie_Statement_2022_3119732739.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "import fitz\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "b__ZH7P2XVgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf_url(pdf_url):\n",
        "    response = requests.get(pdf_url)\n",
        "    if response.status_code == 200:\n",
        "        # Use BytesIO to load the PDF content in memory without saving it\n",
        "        pdf_content = BytesIO(response.content)\n",
        "\n",
        "        pdf_document = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
        "\n",
        "        pdf_text = \"\"\n",
        "        for page_num in range(pdf_document.page_count):\n",
        "            page = pdf_document.load_page(page_num)  # Load each page\n",
        "            pdf_text += page.get_text()  # Extract text from the page\n",
        "\n",
        "        pdf_document.close()  # Close the PDF\n",
        "        return pdf_text\n",
        "    else:\n",
        "        print(f\"Failed to retrieve PDF from {pdf_url}\")\n",
        "        return None\n",
        "\n",
        "pdf_url = 'https://files.ebrains.eu/file/1915ab0c-b800-46ca-b433-75f0e7d07dcf/ProjectCommunicationsContentOfficer-Posting.pdf'\n",
        "pdf_text = extract_text_from_pdf_url(pdf_url)\n",
        "\n",
        "if pdf_text:\n",
        "    print(\"Extracted PDF text:\")\n",
        "    print(pdf_text)  # Print the first 1000 characters of the PDF text"
      ],
      "metadata": {
        "id": "sPRcJDRjXqYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def process_pdf_text(pdf_text):\n",
        "    # Step 1: Remove extra newlines and replace them with spaces\n",
        "    # processed_text = pdf_text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "    # Step 2: Remove extra spaces (more than one space in a row)\n",
        "    processed_text = re.sub(r'\\s+', ' ', pdf_text).strip()\n",
        "\n",
        "    # Step 3: Optional - Remove specific unwanted patterns like \"Page X of Y\" if present\n",
        "    # Example: Remove occurrences of \"Page X of Y\"\n",
        "    processed_text = re.sub(r'Page \\d+ of \\d+', '', processed_text)\n",
        "\n",
        "    # Step 4: Optional - Remove other patterns, such as timestamps, headers, or footers\n",
        "    # Example: Remove timestamps like [00:12:34]\n",
        "    processed_text = re.sub(r'\\[\\d{2}:\\d{2}:\\d{2}\\]', '', processed_text)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Example use with extracted PDF text\n",
        "pdf_url = 'https://files.ebrains.eu/file/1915ab0c-b800-46ca-b433-75f0e7d07dcf/ProjectCommunicationsContentOfficer-Posting.pdf'\n",
        "pdf_text = extract_text_from_pdf_url(pdf_url)\n",
        "\n",
        "if pdf_text:\n",
        "    # Process the extracted text\n",
        "    clean_text = process_pdf_text(pdf_text)\n",
        "\n",
        "    # Output the cleaned text\n",
        "    print(\"Processed PDF text:\")\n",
        "    print(clean_text)  # Print first 1000 characters of the cleaned text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4pILYzbYYf6",
        "outputId": "3a3541c8-2317-49d3-e652-cd580d83f29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed PDF text:\n",
            "EBRAINS, International non-profit association, Chaussée de la Hulpe 166, “Glaverbel”, First Floor, Section B, 1170 Brussels, Belgium Company registration number 0740.908.863 - VAT BE 0740.908.863 | Banque/Bank/Bank Account: IBAN: BE31 7360 6257 3855-BIC: KREDBEBB www.ebrains.eu @EBRAINS_eu EBRAINS PROJECT COMMUNICATIONS & CONTENT OFFICER EBRAINS is a European digital research infrastructure for brain research developed by the EC-funded Human Brain Project. Its ambition is to be an enabler of brain science in Europe, offering state-of-the art resources of data, models, tools and computing platforms to the scientific community. In order to build EBRAINS’ presence and profile among the scientific community at large and the wider public in Europe and other parts of the world, EBRAINS is looking for a Project Communications & Content Officer. EBRAINS is currently further developing its Open Science Tools offering, gaining maturity and robustness, improving its user experience, and broadening the community of neuroscientists and developers who benefit from its resources. In 2021, EBRAINS was added to the ESFRI Roadmap for Research Infrastructures and now also aims to expand collaborations with other EU research infrastructures. The EBRAINS AISBL is a non-profit association and coordinates the operation of the EBRAINS European research infrastructure on behalf of its members. You will be mainly active in defining content and (digital) media priorities and in coordinating these activities while being also involved in all workflows and contributing to the broader EBRAINS communications activities when required. The position reports to EBRAINS Communications Manager and is based in Brussels. Responsibilities • Write engaging content based on the research powered by EBRAINS that appeals to various target audiences and is adapted to the medium through which it is communicated (e.g., website, social media, newsletters, corporate presentations, events) • Identify stories opportunities coming from our partners, researchers, and users • Support the monitoring and reporting of communications plans • Coordinate communications projects with external partners or service providers • Support the website updates • Collaborate for the development of dissemination material for EBRAINS (e.g. podcasts, videos, visuals) • Prospect, segment, and target lead lists by industry or specific characteristics to aid in campaigning efforts • Support various dissemination tasks of EU-funded research projects Qualifications • Hold a bachelor’s degree (minimum) in a natural science related to EBRAINS’ areas of research and/or in Marketing, Communications, Science Communication, Journalism or English Language and Literature • 3-5 years of communications experience, preferable in science journalism or press departments of research-related organisations • Very strong English language skills, both written and spoken. Native English level is a plus • Stellar writing skills and track record of drafting science-communication content for different communications channels (e.g., website, social media, press releases) • Experience in the research, health, or med-tech sector is a plus • A can-do attitude, eagerness to grow and to learn, and will do so proactively by own initiative through the guidance of manager and team members • Good command of MS Office (Word, Excel, and PowerPoint), Adobe Suite (InDesign, Photoshop, Premier) EBRAINS, International non-profit association, Chaussée de la Hulpe 166, “Glaverbel”, First Floor, Section B, 1170 Brussels, Belgium Company registration number 0740.908.863 - VAT BE 0740.908.863 | Banque/Bank/Bank Account: IBAN: BE31 7360 6257 3855-BIC: KREDBEBB www.ebrains.eu @EBRAINS_eu EBRAINS • Digital skills (social media management platforms, Google Analytics, media monitoring software, CRM, CMS…) • Have attention to detail, multitask and deliver on time • Practical, proactive, team player with a positive attitude Why work at the EBRAINS AISBL? Understanding the human brain is one of the greatest challenges facing 21st century science. Joining the EBRAINS AISBL will put you at the centre of an effort to gain profound insights into the structure and functioning of the brain, develop new treatments for brain diseases and build revolutionary new computing technologies. The EBRAINS AISBL team plays a central role in the EBRAINS research infrastructure in Europe and worldwide. You will find yourself inspired by the exciting things that people in the project are building and make a meaningful contribution to understanding the brain. We strive to find the right people and to keep our skills and insights sharp. We are a young, dynamic, interdisciplinary and international team. We focus on delivering high value to the scientific community and are involved in everything from community engagement to integrating scientific workflows with some of the world’s largest supercomputers. Work in the team involves open feedback, with opportunities for additional training to improve our skills. Join us and make an important contribution to the success of one of the most exciting projects of the 21st century! Start date: as soon as possible. Activity rate: 100% Duration of contract: 2 years Applicants should submit a motivation letter and a detailed CV in a single PDF format only, with file name “Surname_position applied_motivation letter and CV” electronically to jobs@ebrains.eu. Please use the position title in the “subject” field. Applications that do not comply with this request will not be considered.\n"
          ]
        }
      ]
    }
  ]
}